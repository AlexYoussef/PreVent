{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWS and ML classifier evalutaion on COVID respiratory deterioration from EHR\n",
    "\n",
    "Code to evaluate various EWS and ML classifiers on predicting respiratory deterioration (surrogated by escalation in repiratory care apparatus/ICU admission) in COVID patients, given static EHR readings for the patient.\n",
    "\n",
    "Logic:\n",
    "\n",
    "- Nested 5-fold CV on model and all data over hyperparameters for model. Scored via 'roc_auc'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from cycler import cycler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt; plt.ioff()\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import confusion_matrix, get_scorer, make_scorer, recall_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "from sklearn.experimental import enable_iterative_imputer; from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from autoimpute.imputations import SingleImputer, MultipleImputer\n",
    "\n",
    "import ews_thresholds as ews\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def mean_confidence_interval(a, sd=None, n=None, confidence=0.95, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of values, returns a string of form: \n",
    "    'mean (confidence, interval)'\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(a)\n",
    "        mean = np.mean(a)\n",
    "        se = scipy.stats.sem(a)\n",
    "        h = se * scipy.stats.t.ppf((1 + confidence) / 2, n-1)\n",
    "        left = mean - h\n",
    "        right = mean + h\n",
    "        \n",
    "    else:\n",
    "        mean = a\n",
    "        left, right = scipy.stats.norm.interval(alpha=confidence, loc=mean, scale=sd/np.sqrt(n))\n",
    "    \n",
    "    if verbose:\n",
    "        return f'{mean:.2f} ({left:.2f}-{right:.2f})'\n",
    "    else:\n",
    "        return mean, (mean - h, mean + h)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def reliability_measures(pred, actual):\n",
    "    \"\"\"\n",
    "    Given a set or predicted values and their true values, returns\n",
    "    the accuracy, sensitivity, specificity, and precision.\n",
    "    \n",
    "    Given the measures are binary and symmetric, the order of pred/actual\n",
    "    can be reversed with no change in behaviour.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(pred, actual).ravel()\n",
    "    acc = (tn+tp) / (tn+fp+fn+tp)\n",
    "    sen = tp / (tp+fn)\n",
    "    sps = tn / (tn+fp)\n",
    "    prs = tp / (tp+fp)\n",
    "    \n",
    "    return acc, sen, sps, prs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def data_imputing(data, method='median', baseline_method='mean'):\n",
    "    \"\"\"\n",
    "    Data imputation function.\n",
    "    \"\"\"\n",
    "    estimator_dict = {'RF': RandomForestRegressor(max_depth=2, random_state=random_state),\n",
    "                      'SVR': SVR(C=1.0, epsilon=0.3),\n",
    "                      'GBT': GradientBoostingRegressor(n_estimators=10, random_state=random_state),\n",
    "                      'GPR': GaussianProcessRegressor(kernel=RBF(0.1) + WhiteKernel(0.01), n_restarts_optimizer=9),\n",
    "                      'BayesianRidge': BayesianRidge()}\n",
    "    \n",
    "    if method in ('mean', 'mode', 'median', 'norm'):\n",
    "        imputer = SingleImputer(strategy=method)\n",
    "    elif method in ('least squares', 'stochastic', 'multinomial logistic'):\n",
    "        imputer = MultipleImputer(1, strategy=method, return_list=True)\n",
    "    elif method == 'KNN':\n",
    "        imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "    elif method in ('BayesianRidge', 'RF', 'GBT', 'SVR', 'GPR'):\n",
    "        imputer = IterativeImputer(random_state=random_state, estimator=estimator_dict[method])\n",
    "    \n",
    "    # Imputing baseline values with population means\n",
    "    if baseline_method:\n",
    "        bl_imputer = SingleImputer(strategy=baseline_method)\n",
    "        bl_cols = [col for col in data.columns if 'Baseline' in col]\n",
    "        data[bl_cols] = bl_imputer.fit_transform(data[bl_cols])\n",
    "    \n",
    "    # Imputing values with method\n",
    "    data = imputer.fit_transform(data)\n",
    "    \n",
    "    if method in ('least squares', 'stochastic', 'multinomial logistic'):\n",
    "        data = data[0][1]\n",
    "        \n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def data_manipulation(data, imputation_method='median', include_fio2=False):\n",
    "    \"\"\"\n",
    "    Data imputation and unit adjustment.\n",
    "    \"\"\"\n",
    "    # Imputing numeric columns\n",
    "    if not include_fio2:\n",
    "        data.drop(['Blood_Gas FIO2'], axis=1, inplace=True)\n",
    "        # [col for col in data.columns if 'fio2' in col.lower()]\n",
    "    data.loc[:,\"Vital_Signs masktyp\"].fillna(value=0, inplace=True) #important point as our prediction is based on the masktype (level of care)\n",
    "    gen = data[[\"hadm_id\", \"sex\"]]\n",
    "    data.drop(\"sex\", axis=1, inplace=True) # Drop gender while imputing\n",
    "    data = data_imputing(data, imputation_method)\n",
    "    data = pd.merge(data, gen, on=\"hadm_id\") # Re-add gender\n",
    "    data = data.drop_duplicates().reset_index().drop(\"index\", axis=1)\n",
    "    \n",
    "    data['sex'] = data['sex'].replace({\"M\":0, \"F\":1})\n",
    "    data.loc[data['Vital_Signs avpu'] == -1, 'Vital_Signs avpu'] = 0\n",
    "\n",
    "    # for the blood_test_gas and all_features_etc combined feature sets\n",
    "    \n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def filter_feature(df, feature_type, top_n_features=False):\n",
    "    \"\"\"\n",
    "    Given complete dataset, outputs subset of features (with label col).\n",
    "    \"\"\"\n",
    "    \n",
    "    if top_n_features:\n",
    "        return df[top_n_features + ['label']].copy()\n",
    "    \n",
    "    # Grabbing feature-sets' columns\n",
    "    EWS_bloods = ['Blood_Test Albumin-g/L','Blood_Test Creatinine-umol/L', 'Blood_Test Haemoglobin-g/dL',\n",
    "                  'Blood_Test Urea-mmol/L', 'Blood_Test WhiteCells-x10^9/L',\n",
    "                  # Does it matter which is used?\n",
    "                  'Blood_Test Potassium-mmol/L', 'Blood_Test Sodium-mmol/L']\n",
    "    CEWS_vitals = ['Vital_Signs HR', 'Vital_Signs RR', 'Vital_Signs SBP', 'Vital_Signs SPO2', 'Vital_Signs TEMP']\n",
    "    NEWS_vitals = CEWS_vitals + ['Vital_Signs avpu', 'Vital_Signs masktyp']\n",
    "    \n",
    "    EWS_misc = ['age', 'sex', 'hrs_from_adm_lab']\n",
    "    \n",
    "    vital_baseline_delta = [c for c in df.columns if 'Vital_Signs Baseline' in c]\n",
    "    baseline_delta = [c for c in df.columns if 'Baseline' in c]\n",
    "    variations = [c for c in df.columns if ('Delta_Mean' in c or 'Var_Mean' in c or 'Max_Min' in c) and c not in baseline_delta]\n",
    "    \n",
    "    vitals = [c for c in df.columns if 'Vital_Signs' in c and c not in baseline_delta and c not in variations]\n",
    "    bloodgas = [c for c in df.columns if 'Blood_Gas' in c and c not in baseline_delta and c not in variations]\n",
    "    bloodtest = [c for c in df.columns if 'Blood_Test' in c and c not in baseline_delta and c not in variations]\n",
    "    allfeats = vitals + bloodtest + bloodgas + EWS_misc\n",
    "    \n",
    "    aews_feats = NEWS_vitals + ['age']\n",
    "    ldtews_feats = EWS_bloods + ['sex']\n",
    "    ldtews_news_feats = NEWS_vitals + ldtews_feats + ['hrs_from_adm_lab']\n",
    "    \n",
    "    \n",
    "    feature_cols_dict = {'Vitals': vitals,\n",
    "                         'Blood_Test': bloodtest,\n",
    "                         'Blood_Gas': bloodgas,\n",
    "                         \n",
    "                         'Vitals&Vars': vitals + variations,\n",
    "                         'Vitals&Vars&Baseline_Delta': vitals + variations + vital_baseline_delta,\n",
    "                         \n",
    "                         'All Features': allfeats,\n",
    "                         'All Features&Vars': allfeats + variations,\n",
    "                         'All Features&Vars&Baseline_Delta': allfeats + variations + baseline_delta,\n",
    "                         \n",
    "                         'EWS_cols': NEWS_vitals + EWS_bloods + EWS_misc,\n",
    "                         'EWS_Bloods': EWS_bloods,\n",
    "                         'NEWS_feats': NEWS_vitals,\n",
    "                         'CEWS_feats': CEWS_vitals,\n",
    "                         'MCEWS_feats': NEWS_vitals,\n",
    "                         'AEWS_feats': aews_feats,\n",
    "                         'LDTEWS_feats': ldtews_feats,\n",
    "                         'LDTEWS_NEWS_feats': ldtews_news_feats}\n",
    "    \n",
    "    filtered_df = df[feature_cols_dict[feature_type] + ['label'].copy()\n",
    "    \n",
    "    return filtered_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def misclass_stats(test_data, pred_acc):\n",
    "    \"\"\"\n",
    "    Saves two files - one wth the ID's of patients correctly classified on last run of test-data \n",
    "    of last model/feature set tested;\n",
    "    and one with that of those incorrectly classified.\n",
    "    \n",
    "    Ethnicity codes available at:\n",
    "    https://peoplefirst.nhsbt.nhs.uk/People%20First%20-%20Document%20Library/Pay/General%20-%20Ethnic%20origin%20codes%202015.pdf\n",
    "    \n",
    "    white = ['A','B','C']\n",
    "    unknowns = [99,'Z'] \n",
    "    bame = [code for code in codelist if code not in white and not in unknowns]\n",
    "    \n",
    "    # 99 is Unknown: https://www.datadictionary.nhs.uk/data_elements/ethnic_category.html\n",
    "    # 'Z' is unstated: https://www.datadictionary.nhs.uk/attributes/ethnic_category_code_2001.html\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    testdf = test_data.copy()\n",
    "\n",
    "    testdf[\"pred_lbls\"] = pred_acc\n",
    "    testdf[\"slicer\"] = testdf.label != testdf.pred_lbls\n",
    "    \n",
    "    misclassified = testdf[testdf.slicer==1][[\"hadm_id\",\"age\",\"sex\"]]\n",
    "    correctly_classified = testdf[testdf.slicer==0][[\"hadm_id\",\"age\",\"sex\"]]\n",
    "    \n",
    "    misclassified.to_csv(file_loc / 'results' / \"misclassified_gen_age.csv\")\n",
    "    correctly_classified.to_csv(file_loc / 'results' / \"correctly_classified_gen_age.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_mean_auroc(tprs, aucs, method, f_type, ax=None, save_fig=True):\n",
    "    \"\"\"\n",
    "    Given multiple ROC performances, plots mean performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    if ax==None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr,\n",
    "            label=f'Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f}) {method}',\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    \n",
    "    if 'EWS' not in method or method=='LDTEWS_NEWS':\n",
    "        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    else:\n",
    "        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=None)\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def nested_cv(X, y, model, n_repeats=40, oversample=True):\n",
    "    \"\"\"\n",
    "    Given model type, and default hyperparameter list, performs nested CV (5x5) grid search.\n",
    "    \n",
    "    Outputs outer CV results for each fold on multiple metrics (acc, sen, sps, prs, auc etc).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define models & params\n",
    "    t = {'n_estimators': [10, 20, 50, 100, 500, 1000]}\n",
    "    \n",
    "    model_params = {'LR': {'C': [1, 3, 5, 10, 100]}, 'MLP': {'hidden_layer_sizes': [10, 30, 50, 100, 200]}, \n",
    "                    'RF':t, 'GBT':t, 'BRF':t, 'BBag':t}\n",
    "    \n",
    "    model_clfs = {'LR': LogisticRegression(random_state=random_state),\n",
    "                  'RF': RandomForestClassifier(random_state=random_state), \n",
    "                  'GBT': GradientBoostingClassifier(random_state=random_state), \n",
    "                  'BRF': BalancedRandomForestClassifier(random_state=random_state), \n",
    "                  'MLP': MLPClassifier(random_state=random_state),\n",
    "                  'BBag': BalancedBaggingClassifier(random_state=random_state)}\n",
    "    \n",
    "    clf = model_clfs[model]\n",
    "    \n",
    "    # Grab \"hypo\"-parameters for nested CV\n",
    "    param_grid = model_params[model]\n",
    "    \n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state) \n",
    "    \n",
    "    # Create pipeline to do class-balancing in internal CV\n",
    "    if oversample:\n",
    "        outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        oversampler = RandomOverSampler(sampling_strategy='minority', random_state=random_state)\n",
    "        pipeline = make_pipeline(oversampler, clf)\n",
    "        param_grid = {f'{type(clf).__name__.lower()}__{key}':value for key, value in param_grid.items()}\n",
    "    else:\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeats, shuffle=True, random_state=random_state)\n",
    "        pipeline = clf\n",
    "        \n",
    "    # Scaling data to encourage alg convergence\n",
    "    if method=='LR':\n",
    "        X = scale(X)\n",
    "    \n",
    "    # Define metrics for scoring\n",
    "    scoring = {'AUC': 'roc_auc', \n",
    "               'Accuracy': get_scorer('accuracy'),\n",
    "               'Precision': get_scorer('precision'),\n",
    "               'Sensitivity': get_scorer('recall'),\n",
    "               'Specificity': make_scorer(recall_score, pos_label=0),\n",
    "               'Balanced_accuracy': get_scorer('balanced_accuracy')}\n",
    "    \n",
    "    # Non_nested parameter search and scoring\n",
    "    clf = GridSearchCV(pipeline, param_grid, cv=inner_cv, scoring=scoring, refit='AUC')\n",
    "    clf.fit(X, y)\n",
    "    print('Inner CV completed.')\n",
    "    \n",
    "    # Nested results\n",
    "    nested_scores = cross_validate(clf, X, y, cv=outer_cv, scoring=scoring, return_estimator=True)\n",
    "    print('Outer CV completed.')\n",
    "        \n",
    "    return nested_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def classification_test(Xtrain, ytrain, Xtest, ytest, model, EWS_threshold):\n",
    "    \"\"\"\n",
    "    Grabs appropriate classifier based on model name and applies to train data.\n",
    "    \"\"\"\n",
    "    \n",
    "    score_test = ews.calculate_ews(Xtest, model)\n",
    "    prob_test = score_test[f'{model}_prob']\n",
    "    \n",
    "    fpr, tpr, _ = metrics.roc_curve(ytest, prob_test)\n",
    "    roc_auc_test = metrics.auc(fpr, tpr)\n",
    "    test = {'sen':[], 'sps':[], 'acc':[], 'prs':[], 'roc_auc':roc_auc_test, 'thresh':[]}\n",
    "    \n",
    "    if EWS_threshold == 'default':\n",
    "        \n",
    "        default_thresh = score_thresh_dict[model]['default']\n",
    "        pred_test = score_test[model]>=default_thresh\n",
    "\n",
    "        acc, sen, sps, prs = reliability_measures(ytest, pred_test)\n",
    "        \n",
    "        test['acc'].append(acc)\n",
    "        test['sen'].append(sen)\n",
    "        test['sps'].append(sps)\n",
    "        test['prs'].append(prs)\n",
    "        test['thresh'].append(default_thresh)\n",
    "        \n",
    "        test['fpr'] = fpr\n",
    "        test['tpr'] = tpr\n",
    "        \n",
    "        return test\n",
    "\n",
    "    # Optimising thresholds\n",
    "    elif EWS_threshold == 'optimise':\n",
    "        \n",
    "        # Note: not (necessarily) a probability - the value output by the EWS (e.g. between 0 and 20)\n",
    "        score_train = ews.calculate_ews(Xtrain, model)\n",
    "        prob_train = score_train[f'{model}_prob']\n",
    "        \n",
    "        # ROC_AUC doesn't vary with threshold\n",
    "        fpr, tpr, _ = metrics.roc_curve(ytrain, prob_train)\n",
    "        roc_auc_train = metrics.auc(fpr, tpr)\n",
    "\n",
    "        # Containers to hold lists of values for each train/test run for each EWS threshold\n",
    "        train = {'sen':[], 'sps':[], 'bal_acc':[], 'acc':[], 'prs':[], 'roc_auc':roc_auc_train, 'thresh':[]}\n",
    "        \n",
    "        # Calculate metrics for each threshold\n",
    "        for thresh in score_thresh_dict[model]['range']:\n",
    "            \n",
    "            # On train\n",
    "            pred_train = score_train[model]>=thresh\n",
    "            acc, sen, sps, prs = reliability_measures(ytrain,pred_train)\n",
    "            bal_acc = (sen+sps)/2\n",
    "            \n",
    "            train['bal_acc'].append(bal_acc)\n",
    "            train['acc'].append(acc)\n",
    "            train['sen'].append(sen)\n",
    "            train['sps'].append(sps)\n",
    "            train['prs'].append(prs)\n",
    "            train['thresh'].append(thresh)\n",
    "            \n",
    "            # On test\n",
    "            pred_test = score_test[model]>=thresh\n",
    "            acc, sen, sps, prs = reliability_measures(ytest,pred_test)\n",
    "            \n",
    "            test['acc'].append(acc)\n",
    "            test['sen'].append(sen)\n",
    "            test['sps'].append(sps)\n",
    "            test['prs'].append(prs)\n",
    "            test['thresh'].append(thresh)\n",
    "\n",
    "        return train, test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def main(raw_data,feature_types,methods,EWS_threshold='default',plot_curves=False,split_ids=False,top_n_features=None):\n",
    "    \"\"\"\n",
    "    Main program. For each:\n",
    "    \n",
    "    - Feature set\n",
    "    - ML method\n",
    "    \n",
    "    1. Takes the input data\n",
    "    2. Appropriately subsets its columns\n",
    "    3. Performes (Nested) GridSearchCV (data-balancing / hyper-param tuning / model training)\n",
    "    7. Outputs avg performance on test sets across folds in CV.\n",
    "    \n",
    "    If the method is an EWS system, depending on EWS_threshold value will either:\n",
    "    \n",
    "    - Output performance of EWS on above CV's test sets.\n",
    "    - Output performance of EWS with (balanced) accuracy-optimised threshold (as per GridSearchCV logic) on above.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_df = pd.DataFrame({\"feature_type\":[], \"method\":[], \"acc\":[], \"sen\":[],\"sps\":[], \"prs\":[], \"AUC\":[]})\n",
    "\n",
    "    for f_type in tqdm(feature_types, desc='Feature set'):\n",
    "        data = filter_feature(raw_data, f_type, top_n_features)\n",
    "        if plot_curves:\n",
    "            fig, ax = plt.subplots()\n",
    "        \n",
    "        for method in tqdm(methods, desc='Model', position=1, leave=bool(f_type==feature_types[-1])): \n",
    "            n_splits=5\n",
    "            skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "            \n",
    "            ####################################\n",
    "            # Perform Nested GridSearchCV for ML models\n",
    "            ####################################\n",
    "            if 'EWS' not in method:\n",
    "                X = data.drop('label', axis=1).values\n",
    "                y = data['label'].values.ravel()\n",
    "                \n",
    "                nested_scores = nested_cv(X, y, method)\n",
    "                \n",
    "                accs = nested_scores['test_Accuracy']\n",
    "                sens = nested_scores['test_Sensitivity']\n",
    "                spss = nested_scores['test_Specificity']\n",
    "                prss = nested_scores['test_Precision']\n",
    "                aucs = nested_scores['test_AUC']\n",
    "                \n",
    "                results_df = results_df.append({\"feature_type\":f_type,\"method\":method,\n",
    "                                                \"acc\":mean_confidence_interval(accs),\n",
    "                                                \"sen\":mean_confidence_interval(sens),\n",
    "                                                \"sps\":mean_confidence_interval(spss),\n",
    "                                                \"prs\":mean_confidence_interval(prss),\n",
    "                                                \"AUC\":mean_confidence_interval(aucs)},\n",
    "                                               ignore_index=True)\n",
    "                \n",
    "                # Re-iterating through test-data splits to calculate fpr/tpr for plotting ROC curves\n",
    "                if plot_curves:\n",
    "                    tprs = []\n",
    "                    for i, (train_index, test_index) in enumerate(skf.split(data, data['label'])):\n",
    "                        X_test = X[test_index]\n",
    "                        y_test = y[test_index]\n",
    "                        \n",
    "                        clf_fold = nested_scores['estimator'][i]\n",
    "\n",
    "                        viz = metrics.plot_roc_curve(clf_fold, X_test, y_test, name=f'ROC fold {i}',\n",
    "                                                     alpha=0.3, lw=1, ax=ax)\n",
    "                        \n",
    "                        mean_fpr = np.linspace(0, 1, 100)\n",
    "                        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "                        interp_tpr[0] = 0.0\n",
    "                        tprs.append(interp_tpr)\n",
    "\n",
    "            ####################################\n",
    "            # Perform CV for EWS \n",
    "            # (using same random seed to ensure comparative testing with ML results)\n",
    "            ####################################\n",
    "            else:\n",
    "                data = filter_feature(raw_data, f'{method}_feats')\n",
    "                # Output of metrics on train/test for optimising threshes\n",
    "                train_folds = []\n",
    "                test_folds = []\n",
    "                \n",
    "                # Need to manually go through each fold for EWS since they are not defined as classifiers\n",
    "                for i, (train_index, test_index) in tqdm(enumerate(skf.split(data, data['label'])), desc=f'Stratified {n_splits}-fold', total=n_splits, leave=False):\n",
    "                    train_data, test_data = data.loc[train_index,:], data.loc[test_index,:]\n",
    "\n",
    "                    # Split labels and features\n",
    "                    ytrain = train_data[['label']].values\n",
    "                    ytest = test_data[['label']].values\n",
    "                    \n",
    "                    # Keep column names for EWS calculations\n",
    "                    Xtrain = train_data.drop('label', axis=1)\n",
    "                    Xtest = test_data.drop('label', axis=1)\n",
    "                    \n",
    "                    # Calculating performance metrics\n",
    "                    if EWS_threshold == 'default':\n",
    "                        test = classification_test(Xtrain,ytrain,Xtest,ytest,method,EWS_threshold)\n",
    "                        \n",
    "                        if plot_curves:\n",
    "                            # Plotting fold's ROC\n",
    "                            viz = metrics.RocCurveDisplay(fpr=test['fpr'], tpr=test['tpr'], roc_auc=test['roc_auc'], \n",
    "                                                          estimator_name=f'ROC fold {i}')#,\n",
    "                                                          #alpha=0.3, lw=1, ax=ax)\n",
    "\n",
    "                            mean_fpr = np.linspace(0, 1, 100)\n",
    "                            interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "                            interp_tpr[0] = 0.0\n",
    "                            test['interp_tpr'] = interp_tpr\n",
    "                        \n",
    "                        test_folds.append(test)\n",
    "    \n",
    "                    elif EWS_threshold == 'optimise':\n",
    "                        train, test = classification_test(Xtrain,ytrain,Xtest,ytest,method,EWS_threshold)\n",
    "                        train_folds.append(train)\n",
    "                        test_folds.append(test)\n",
    "\n",
    "                if EWS_threshold == 'default':\n",
    "                    i=0\n",
    "                    \n",
    "                elif EWS_threshold == 'optimise':\n",
    "                    # Get mean performance of each threshold across train folds\n",
    "                    mean_acc = [np.mean([fold['bal_acc'][j] for fold in train_folds]) for j in range(len(train_folds[0]['acc']))]\n",
    "                    \n",
    "                    # Grab best performing thresh on train\n",
    "                    i = np.argmax(mean_acc)\n",
    "                \n",
    "                # Optimised/default threshold\n",
    "                thresh = test_folds[0]['thresh'][i]\n",
    "                    \n",
    "                # Optimised/default metrics over all folds\n",
    "                acc = [fold['acc'][i] for fold in test_folds]\n",
    "                sen = [fold['sen'][i] for fold in test_folds]\n",
    "                sps = [fold['sps'][i] for fold in test_folds]\n",
    "                prs = [fold['prs'][i] for fold in test_folds]\n",
    "                roc_auc = [fold['roc_auc'] for fold in test_folds]\n",
    "                \n",
    "                # Add results to output\n",
    "                results_df = results_df.append({\"feature_type\":f_type,\n",
    "                                                \"method\":method,\n",
    "                                                \"threshold\":thresh,\n",
    "                                                \"acc\":mean_confidence_interval(acc), \n",
    "                                                \"sen\":mean_confidence_interval(sen), \n",
    "                                                \"sps\":mean_confidence_interval(sps), \n",
    "                                                \"prs\":mean_confidence_interval(prs), \n",
    "                                                \"AUC\":mean_confidence_interval(roc_auc)},\n",
    "                                               ignore_index=True)\n",
    "                    \n",
    "                \n",
    "            # Plot mean AUROC\n",
    "            if plot_curves:\n",
    "                if 'EWS' in method:\n",
    "                    tprs = [fold['interp_tpr'] for fold in test_folds]\n",
    "                    aucs = [fold['roc_auc'] for fold in test_folds]\n",
    "                    \n",
    "                plot_mean_auroc(tprs, aucs, method, f_type, ax=ax)\n",
    "                \n",
    "            feature_names = [col for col in data.columns.tolist() if col != 'label']\n",
    "            #fimp_df = pd.DataFrame({\"feature\":feature_names, \"feature_weight\":importance})\n",
    "            #fimp_df.sort_values(\"feature_weight\",ascending=False,inplace=True)\n",
    "                \n",
    "    if False:\n",
    "        misclass_stats(test_data, results['pred'])\n",
    "    \n",
    "    if plot_curves:\n",
    "        plt.show()\n",
    "        if 'EWS' in method:\n",
    "            plt.savefig(file_loc / 'imgs' / 'EWS_roc.png')\n",
    "        else:\n",
    "            plt.savefig(file_loc / 'imgs' / '{method}_{f_type}.png')\n",
    "    \n",
    "    return results_df #, fimp_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 0a: Assess performance of EWS systems (default thresholds)\n",
    "* Step 0b: Assess performance of EWS systems (thresholds optimised for balanced-accuracy on 5-fold CV)\n",
    "* Step 1: Compare multiple ML models on EWS feature-sets to determine best performer.\n",
    "* Step 2: Train and evaluate best performing model on extended feature sets.\n",
    "* Step 3: ~Plot AUROC curve.~ TO IMPLEMENT AVG ACROSS CV\n",
    "* Step 4: ~Display most important features.~ TO IMPLEMENT AVG ACROSS CV\n",
    "* Step 5: Additional feature sets / data set evaluations (age, different lookack windows etc)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Global seed for reproducibility\n",
    "random_state = 4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Loading raw data\n",
    "file_loc = Path('Z:/netshares/ibme_chi/Projects_1/orchid/processing/SK/JA')\n",
    "data = pd.read_csv(file_loc / 'data' / 'dataevents_updated2_N24_W24.csv')\n",
    "data = data_manipulation(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0a: Assess performance of EWS systems (default thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Threshold ranges for each model\n",
    "score_thresh_dict = {\"NEWS\": {'default':5, 'range':np.arange(0,21)},\n",
    "                     \"MCEWS\": {'default':4, 'range':np.arange(0,21)}, \n",
    "                     \"CEWS\": {'default':4, 'range':np.arange(0,18)}, \n",
    "                     \"AEWS\": {'default':5, 'range':np.arange(0,20)}, \n",
    "                     \"LDTEWS\": {'default':0.33, 'range':np.arange(0,1,0.01)},\n",
    "                     \"LDTEWS_NEWS\": {'default':0.27, 'range':np.arange(0,1,0.01)}}\n",
    "\n",
    "# EWS models and set of columns for calculations\n",
    "ews_methods = score_thresh_dict.keys()\n",
    "ews_feature_types = ['EWS_cols']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ews_df = main(data, ews_feature_types, ews_methods, plot_curves=True)\n",
    "ews_df\n",
    "# use df.to_latex(index=False) for exporting of results for paper"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0b: Assess performance of EWS systems (thresholds optimised for balanced-accuracy on 5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "opt_ews_df = main(data, ews_feature_types, ews_methods, EWS_threshold='optimise')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Compare multiple ML models on EWS feature-sets to determine best performer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "####################\n",
    "# imputation_methods\n",
    "####################\n",
    "imputation_methods = ['BayesianRidge', 'stochastic', 'mean', 'median'] \n",
    "\n",
    "##############\n",
    "# Feature sets\n",
    "##############\n",
    "# For comparison with EWS\n",
    "feature_types_ews = ['CEWS_feats', 'NEWS_feats', 'AEWS_feats', 'LDTEWS_feats', 'LDTEWS_NEWS_feats']\n",
    "#feature_types_simple = ['Vitals', 'EWS_Bloods', 'Vital_Signs&EWS_Bloods']\n",
    "\n",
    "# F1-F8\n",
    "#feature_types = feature_types_simple + ['Blood_Test', 'Blood_Gas', 'Blood_Test_Gas', 'Vitals&Vars', 'All Features']\n",
    "feature_types = ['Vitals', 'Blood_Test', 'Blood_Gas', 'All Readings']\n",
    "\n",
    "# F9-F11\n",
    "feature_types_extended = ['Vitals&Vars', 'All Features&Vars', 'Vitals&Vars&Baseline_Delta', 'All Features&Vars&Baseline_Delta']\n",
    "\n",
    "###########\n",
    "# ML models\n",
    "###########\n",
    "ml_methods = ['LR', 'BRF', 'GBT']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_df = main(data, feature_types_simple, ml_methods)\n",
    "# Getting precision calculation error (divide by zero) on BRF for some folds - looks like it's labeling all 0 in some folds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Best method per feature set, based on mean AUC\n",
    "best_results_df = results_df[results_df.groupby('feature_type')['AUC'].transform('max') == results_df['AUC']]\n",
    "best_results_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_methods = [best_results_df['method'].value_counts().idxmax()]\n",
    "print(f'Method with best average AUC across EWS feature sets: {best_methods[0]}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Train and evaluate best performing model on extended feature sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "main(data, feature_types_simple, best_methods, plot_curves=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_df_extended = main(data, feature_types_extended, best_methods)\n",
    "results_df_extended"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing ML model with comparative Feature Sets for EWS:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing feature set with best performance for additional tests. \n",
    "\n",
    "Since Vital_Signs&Delta performs comparably to extended feature sets, might want to just use it for simplicity of readings/having fewer features relative to data points/ not overfit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_feature_set = results_df_extended[results_df_extended['AUC'] == results_df_extended['AUC'].max()]['feature_type'].to_list()\n",
    "print(f'Best feature set: {best_feature_set}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Plot ROC curve for best featureset/model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "main(data, best_feature_set, best_methods, plot_curves=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Display most important features. TO IMPLEMENT AVG ACROSS CV"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#JA: issue: we are only selecting features from the final fold, and not plotting the other 4\n",
    "\n",
    "# Top 20 features\n",
    "#feats = pd.DataFrame()\n",
    "#feats[['Top 20 features', 'Top feature_weight']] = fimp_df.head(20).reset_index(drop=True)\n",
    "#feats[['Bottom 20 features', 'Bottom feature_weight']] = fimp_df.tail(20).reset_index(drop=True)\n",
    "\n",
    "#top_20_features = list(fimp_df.head(20)['feature'])\n",
    "\n",
    "#feats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Top 10 features\n",
    "#feats_10 = pd.DataFrame()\n",
    "#feats_10[['Top 10 features', 'Top feature_weight']] = fimp_df.head(10).reset_index(drop=True)\n",
    "#feats_10[['Bottom 10 features', 'Bottom feature_weight']] = fimp_df.tail(10).reset_index(drop=True)\n",
    "\n",
    "#feats_10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on top 20 features only (mitigating overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#results_df_threshold_opt = main(data, all_features_delta, best_methods, output, file_loc, top_n_features=top_20_features)\n",
    "#results_df_threshold_opt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Additional feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding age:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "main(data, ['All Features&Delta&age'], best_methods)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional experiment: 6hr and 12hr windows"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_6hr = pd.read_csv(file_loc / 'data' / f\"dataevents_updated2_N6_W24.csv\")\n",
    "data_6hr = data_manipulation(data_6hr)\n",
    "\n",
    "main(data_6hr, best_feature_set, best_methods)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_12hr = = pd.read_csv(file_loc / 'data' / f\"dataevents_updated2_N12_W24.csv\")\n",
    "data_12hr = data_manipulation(data_12hr)\n",
    "\n",
    "main(data_12hr, best_feature_set, best_methods)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance including FiO2 as a variable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_fio2 = pd.read_csv(file_loc / 'data' / f\"dataevents_updated2_N24_W24.csv\")\n",
    "data_fio2 = data_manipulation(data_fio2, include_fio2=True)\n",
    "\n",
    "main(data_fio2, best_feature_set, best_methods)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
